import streamlit as st
import arxiv
from datetime import datetime, timezone, timedelta
from openai import OpenAI
import time
import urllib3
import requests

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AI-powered Paper Seeker",
    page_icon="ğŸ”",
    layout="wide"
)

# æ ‡é¢˜å’Œæ—¶é—´
col1, col2 = st.columns([3, 1])
with col1:
    st.title("ğŸ” AI-powered Paper Seeker")
with col2:
    st.markdown(f"### ğŸ“… {datetime.now().strftime('%Y-%m-%d')}")
    st.caption(f"ğŸ• {datetime.now().strftime('%H:%M:%S')}")

# ç”¨æˆ·ç ”ç©¶é¢†åŸŸè®¾ç½®
with st.expander("ğŸ‘¤ è®¾ç½®æ‚¨çš„ç ”ç©¶é¢†åŸŸï¼ˆAI å°†ä¼˜å…ˆä¸ºæ‚¨ç²¾é€‰ç›¸å…³è®ºæ–‡ï¼‰", expanded=False):
    user_research_field = st.text_area(
        "è¯·æè¿°æ‚¨çš„ç ”ç©¶æ–¹å‘å’ŒæŠ€æœ¯èƒŒæ™¯",
        value="æˆ‘æ˜¯ä¸€åä»äº‹ç´«å¤–å›ºä½“æ¿€å…‰å™¨çš„æ¿€å…‰å·¥ç¨‹å¸ˆï¼Œä¸»è¦ä»äº‹å‘355nmã€266nmã€193nmç´«å¤–æ¿€å…‰å™¨ï¼ŒåŒæ—¶åœ¨åŠå¯¼ä½“è¡Œä¸šï¼Œæˆ‘è¿˜ä»äº‹è®¾è®¡lithoå››æ³¢é•¿å®šä½æ¿€å…‰å™¨è®¾è®¡ã€‚",
        height=100,
        help="è¯¦ç»†æè¿°æ‚¨çš„ç ”ç©¶æ–¹å‘ã€å…³æ³¨çš„æŠ€æœ¯é¢†åŸŸå’Œåº”ç”¨åœºæ™¯ï¼ŒAI å°†æ ¹æ®æ­¤ä¿¡æ¯ä¸ºæ‚¨ç²¾é€‰æœ€ç›¸å…³çš„è®ºæ–‡"
    )
    st.session_state['user_research_field'] = user_research_field
    if user_research_field:
        st.success("âœ… å·²è®¾ç½®ç ”ç©¶é¢†åŸŸï¼Œæœç´¢æ—¶å°†ä¸ºæ‚¨ç²¾é€‰ç›¸å…³è®ºæ–‡")

# é¡¹ç›®ä»‹ç»
with st.expander("ğŸ“– å…³äºæœ¬æœç´¢å™¨", expanded=False):
    st.markdown("""
    ### ğŸ¯ åŠŸèƒ½ç‰¹è‰²
    æœ¬å·¥å…·æ•´åˆ arXiv é¢„å°æœ¬åº“ä¸ Semantic Scholar APIï¼Œç»“åˆ DeepSeek V3 å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸ºå­¦æœ¯ç ”ç©¶äººå‘˜æä¾›ä¸‰ç§æ™ºèƒ½åŒ–æ£€ç´¢æ¨¡å¼ï¼š
    - **ä»Šæ—¥é€Ÿé€’**ï¼šè‡ªåŠ¨è·å–è¿‡å»24å°æ—¶çš„æœ€æ–°è®ºæ–‡ï¼ŒAI æ™ºèƒ½åˆ†ç±»å¹¶ç”Ÿæˆä¸­æ–‡æ‘˜è¦
    - **é¡¶åˆŠå‘¨è®ºæ–‡**ï¼šè¿½è¸ªå…‰å­¦ä¸è®¡ç®—æœºé¢†åŸŸçš„é¡¶çº§æœŸåˆŠæœ€æ–°å‘è¡¨ï¼ˆNature Photonicsã€Scienceã€CVPRç­‰ï¼‰
    - **è‡ªå®šä¹‰æœç´¢**ï¼šçµæ´»è®¾ç½®æ—¶é—´èŒƒå›´å’Œæ£€ç´¢æ•°é‡ï¼Œæ·±åº¦æŒ–æ˜ç‰¹å®šæ–¹å‘è®ºæ–‡
    
    ### ğŸ” æ•°æ®æ¥æº
    - **arXiv.org**: å…¨çƒæœ€å¤§çš„å¼€æ”¾è·å–é¢„å°æœ¬åº“ï¼Œæ¶µç›–å…‰å­¦ã€è®¡ç®—æœºç§‘å­¦ç­‰å‰æ²¿é¢†åŸŸ
    - **Semantic Scholar**: å¾®è½¯å­¦æœ¯å›¾è°±ï¼Œæä¾›é¡¶åˆŠè®ºæ–‡çš„å¼•ç”¨æ•°æ®å’Œå½±å“åŠ›åˆ†æ
    
    ### ğŸ¤– AI æ™ºèƒ½åˆ†æ
    - **DeepSeek V3 API**: è‡ªåŠ¨åˆ¤æ–­è®ºæ–‡ç ”ç©¶æ–¹å‘ã€ç”Ÿæˆä¸­æ–‡æ‘˜è¦ã€ç­›é€‰ç›¸å…³è®ºæ–‡
    - æ™ºèƒ½ç†è§£è®ºæ–‡å†…å®¹ï¼Œç²¾å‡†åˆ¤æ–­ç ”ç©¶æ–¹å‘ç›¸å…³æ€§
    - è‡ªåŠ¨ç”Ÿæˆä¸­æ–‡æ‘˜è¦ï¼Œå¿«é€Ÿäº†è§£è®ºæ–‡æ ¸å¿ƒå†…å®¹

    """)

st.divider()

# é¡¶åˆŠç™½åå•é…ç½®
TOP_JOURNALS = {
    "å…‰å­¦": [
        "Nature",  # Nature ä¸»åˆŠ
        "Science",  # Science ä¸»åˆŠ
        "Nature Photonics",
        "Nature Communications",
        "Light: Science & Applications",
        "Optica",
        "Physical Review Letters",
        "Advanced Photonics",
        "Laser & Photonics Reviews"
    ],
    "è®¡ç®—æœºç§‘å­¦": [
        "Nature",
        "Science",
        "Nature Machine Intelligence",
        "NeurIPS",
        "ICML",
        "ICLR",
        "CVPR",
        "ICCV"
    ]
}

# å­¦ç§‘é¢†åŸŸé…ç½®
DISCIPLINES = {
    "å…‰å­¦": {
        "icon": "ğŸ”¬",
        "arxiv_categories": ["physics.optics", "eess.IV", "physics.app-ph", "cond-mat.mtrl-sci", "quant-ph"],
        "topics": {
            "éçº¿æ€§å…‰å­¦": {
                "keywords": ["nonlinear", "nonlinear optics", "SHG", "THG", "FWM", "frequency conversion"],
                "description": "éçº¿æ€§å…‰å­¦æ•ˆåº”ã€é¢‘ç‡è½¬æ¢ã€å’Œé¢‘/å·®é¢‘ç­‰"
            },
            "è¶…å¿«å…‰å­¦": {
                "keywords": ["ultrafast", "femtosecond", "picosecond", "attosecond", "pulse"],
                "description": "é£ç§’/çš®ç§’æ¿€å…‰ã€è¶…çŸ­è„‰å†²æŠ€æœ¯"
            },
            "ç´«å¤–æ¿€å…‰": {
                "keywords": ["ultraviolet", "UV laser", "deep-UV", "DUV"],
                "description": "ç´«å¤–/æ·±ç´«å¤–æ¿€å…‰æŠ€æœ¯"
            },
            "é‡å­å…‰å­¦": {
                "keywords": ["quantum optics", "quantum entanglement", "single photon", "quantum state"],
                "description": "é‡å­çº ç¼ ã€å•å…‰å­æºã€é‡å­æ€æ“æ§"
            },
            "å…‰å­¦é¢‘ç‡æ¢³": {
                "keywords": ["optical frequency comb", "frequency comb", "mode-locked", "comb"],
                "description": "å…‰é¢‘æ¢³ã€é”æ¨¡æ¿€å…‰"
            },
            "å…‰çº¤æ¿€å…‰": {
                "keywords": ["fiber laser", "fiber optics", "optical fiber"],
                "description": "å…‰çº¤æ¿€å…‰å™¨ã€å…‰çº¤å…‰å­¦"
            },
            "è¶…è¿ç»­è°±": {
                "keywords": ["supercontinuum", "SC generation", "broadband"],
                "description": "è¶…è¿ç»­è°±äº§ç”Ÿã€å®½å¸¦å…‰æº"
            },
            "å¤ªèµ«å…¹å…‰å­¦": {
                "keywords": ["terahertz", "THz", "terahertz generation"],
                "description": "å¤ªèµ«å…¹äº§ç”Ÿä¸åº”ç”¨"
            },
            "å…‰å‚é‡è¿‡ç¨‹": {
                "keywords": ["OPO", "OPA", "optical parametric", "parametric amplifier"],
                "description": "å…‰å‚é‡æŒ¯è¡å™¨/æ”¾å¤§å™¨"
            },
            "é«˜æ¬¡è°æ³¢": {
                "keywords": ["high harmonic generation", "HHG", "attosecond pulse"],
                "description": "é«˜æ¬¡è°æ³¢äº§ç”Ÿã€é˜¿ç§’è„‰å†²"
            }
        }
    },
    "è®¡ç®—æœºç§‘å­¦": {
        "icon": "ğŸ’»",
        "arxiv_categories": ["cs.AI", "cs.LG", "cs.CV", "cs.CL", "cs.NE", "stat.ML"],
        "topics": {
            "äººå·¥æ™ºèƒ½": {
                "keywords": ["artificial intelligence", "AI", "machine intelligence", "intelligent systems"],
                "description": "é€šç”¨äººå·¥æ™ºèƒ½ã€æ™ºèƒ½ç³»ç»Ÿ"
            },
            "æœºå™¨å­¦ä¹ ": {
                "keywords": ["machine learning", "deep learning", "neural network", "CNN", "RNN", "transformer"],
                "description": "æ·±åº¦å­¦ä¹ ã€ç¥ç»ç½‘ç»œã€æ¨¡å‹è®­ç»ƒ"
            },
            "è®¡ç®—æœºè§†è§‰": {
                "keywords": ["computer vision", "image processing", "object detection", "segmentation", "visual recognition"],
                "description": "å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²"
            },
            "è‡ªç„¶è¯­è¨€å¤„ç†": {
                "keywords": ["natural language processing", "NLP", "language model", "LLM", "GPT", "BERT", "text generation"],
                "description": "è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿ"
            },
            "å¼ºåŒ–å­¦ä¹ ": {
                "keywords": ["reinforcement learning", "RL", "Q-learning", "policy gradient", "deep RL"],
                "description": "å¼ºåŒ–å­¦ä¹ ã€ç­–ç•¥ä¼˜åŒ–ã€æ™ºèƒ½å†³ç­–"
            },
            "ç”Ÿæˆæ¨¡å‹": {
                "keywords": ["generative model", "GAN", "VAE", "diffusion model", "stable diffusion", "image generation"],
                "description": "ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹ã€å›¾åƒç”Ÿæˆ"
            },
            "å¤šæ¨¡æ€å­¦ä¹ ": {
                "keywords": ["multimodal", "vision-language", "CLIP", "cross-modal", "audio-visual"],
                "description": "è§†è§‰-è¯­è¨€ã€è·¨æ¨¡æ€å­¦ä¹ "
            },
            "å›¾ç¥ç»ç½‘ç»œ": {
                "keywords": ["graph neural network", "GNN", "graph learning", "node classification"],
                "description": "å›¾ç¥ç»ç½‘ç»œã€å›¾è¡¨ç¤ºå­¦ä¹ "
            },
            "è”é‚¦å­¦ä¹ ": {
                "keywords": ["federated learning", "distributed learning", "privacy-preserving"],
                "description": "è”é‚¦å­¦ä¹ ã€éšç§ä¿æŠ¤å­¦ä¹ "
            },
            "AIå®‰å…¨": {
                "keywords": ["adversarial", "robust", "trustworthy AI", "AI safety", "explainable AI"],
                "description": "å¯¹æŠ—æ”»å‡»ã€æ¨¡å‹é²æ£’æ€§ã€å¯è§£é‡ŠAI"
            }
        }
    }
}

# åˆå§‹åŒ– DeepSeek API
@st.cache_resource
def init_deepseek():
    return OpenAI(
        api_key="sk-7b13af61c56140dd80595921a087bd27",
        base_url="https://api.deepseek.com",
        timeout=30.0,  # è®¾ç½® 30 ç§’è¶…æ—¶
        max_retries=2   # æœ€å¤šé‡è¯• 2 æ¬¡
    )

client_ai = init_deepseek()

# ç”Ÿæˆä¸­æ–‡æ‘˜è¦ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
def generate_summary(title, abstract, max_attempts=3):
    for attempt in range(max_attempts):
        try:
            response = client_ai.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå…‰å­¦é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡ç®€æ˜æ‰¼è¦åœ°æ€»ç»“è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹å’Œåˆ›æ–°ç‚¹ã€‚"},
                    {"role": "user", "content": f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\n\næ‘˜è¦ï¼š{abstract}\n\nè¯·ç”¨2-3å¥è¯æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ï¼š"}
                ],
                max_tokens=200,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            if attempt < max_attempts - 1:
                time.sleep(2)  # ç­‰å¾… 2 ç§’åé‡è¯•
                continue
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼ˆå·²é‡è¯• {max_attempts} æ¬¡ï¼‰ï¼š{str(e)[:100]}"

# ç”Ÿæˆä¸€å¥è¯æ€»ç»“ï¼ˆç”¨äºä»Šæ—¥é€Ÿé€’ï¼‰
def generate_one_sentence_summary(title, abstract, max_attempts=3):
    client_ai = init_deepseek()
    for attempt in range(max_attempts):
        try:
            response = client_ai.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ç”¨ä¸€å¥è¯ï¼ˆä¸è¶…è¿‡30ä¸ªæ±‰å­—ï¼‰æ¦‚æ‹¬è®ºæ–‡çš„æ ¸å¿ƒå·¥ä½œã€‚"},
                    {"role": "user", "content": f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\n\næ‘˜è¦ï¼š{abstract}\n\nè¯·ç”¨ä¸€å¥è¯è¯´æ˜è¿™ç¯‡è®ºæ–‡åšäº†ä»€ä¹ˆï¼š"}
                ],
                max_tokens=100,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            if attempt < max_attempts - 1:
                time.sleep(2)
                continue
            return "æ€»ç»“ç”Ÿæˆå¤±è´¥"

# åˆ¤æ–­æ˜¯å¦ç›¸å…³å¹¶æ ‡æ³¨å…·ä½“æ–¹å‘ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
def is_relevant_with_tags(title, abstract, topics_list, max_attempts=3):
    # æ„å»ºåŠ¨æ€çš„åˆ¤æ–­æç¤ºè¯
    topics_desc = "ã€".join(topics_list)
    
    for attempt in range(max_attempts):
        try:
            response = client_ai.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯é¢†åŸŸçš„ä¸“å®¶åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­è®ºæ–‡ä¸å“ªäº›ç ”ç©¶æ–¹å‘ç›¸å…³ã€‚"},
                    {"role": "user", "content": f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\n\næ‘˜è¦ï¼š{abstract}\n\nå€™é€‰ç ”ç©¶æ–¹å‘ï¼š{topics_desc}\n\nè¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡ä¸å“ªäº›ç ”ç©¶æ–¹å‘ç›¸å…³ã€‚å¦‚æœç›¸å…³ï¼Œè¯·åˆ—å‡ºå…·ä½“çš„æ–¹å‘åç§°ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼›å¦‚æœéƒ½ä¸ç›¸å…³ï¼Œè¯·å›ç­”'æ— å…³'ã€‚"}
                ],
                max_tokens=50,
                temperature=0.1
            )
            answer = response.choices[0].message.content.strip()
            
            # åˆ¤æ–­æ˜¯å¦ç›¸å…³
            if "æ— å…³" in answer or "ä¸ç›¸å…³" in answer or answer.lower() == "none":
                return False, []
            
            # æå–ç›¸å…³çš„å…·ä½“æ–¹å‘
            related_topics = []
            for topic in topics_list:
                if topic in answer:
                    related_topics.append(topic)
            
            # å¦‚æœæœ‰åŒ¹é…çš„æ–¹å‘ï¼Œè¿”å› True å’Œæ–¹å‘åˆ—è¡¨
            if related_topics:
                return True, related_topics
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®åŒ¹é…ä½†å›ç­”ä¸æ˜¯"æ— å…³"ï¼Œå¯èƒ½æ˜¯ç›¸å…³çš„ï¼Œè¿”å›æ‰€æœ‰å€™é€‰æ–¹å‘
                return True, [topics_list[0]]  # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª
                
        except Exception as e:
            if attempt < max_attempts - 1:
                time.sleep(2)  # ç­‰å¾… 2 ç§’åé‡è¯•
                continue
            st.warning(f"DeepSeek åˆ¤æ–­å¤±è´¥ï¼ˆå·²é‡è¯• {max_attempts} æ¬¡ï¼‰ï¼š{str(e)[:100]}")
            return False, []

# AI ç²¾é€‰è®ºæ–‡ï¼ˆåˆ¤æ–­ä¸ç”¨æˆ·ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ï¼‰
def ai_filter_for_user(title, abstract, user_field, max_attempts=2):
    """
    è®© DeepSeek åˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸ç”¨æˆ·ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³ï¼Œå¹¶æå–å…³é”®å…³è”ç‚¹
    è¿”å›: (æ˜¯å¦ç›¸å…³, ç›¸å…³åº¦è¯„åˆ†1-10, å…³è”è¦ç‚¹)
    """
    if not user_field or user_field.strip() == "":
        return False, 0, ""
    
    client_ai = init_deepseek()
    for attempt in range(max_attempts):
        try:
            response = client_ai.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æ–‡çŒ®åˆ†æä¸“å®¶ã€‚è¯·åˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸ç”¨æˆ·çš„ç ”ç©¶é¢†åŸŸç›¸å…³ï¼Œå¹¶æå–å…³é”®å…³è”ç‚¹ã€‚"},
                    {"role": "user", "content": f"ç”¨æˆ·ç ”ç©¶é¢†åŸŸï¼š{user_field}\n\nè®ºæ–‡æ ‡é¢˜ï¼š{title}\n\nè®ºæ–‡æ‘˜è¦ï¼š{abstract}\n\nè¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡æ˜¯å¦ä¸ç”¨æˆ·ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³ã€‚å¦‚æœç›¸å…³ï¼Œè¯·ç»™å‡ºç›¸å…³åº¦è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰å’Œç®€çŸ­çš„å…³è”è¦ç‚¹ï¼ˆä¸è¶…è¿‡30å­—ï¼‰ã€‚æ ¼å¼ï¼šè¯„åˆ†|å…³è”è¦ç‚¹ã€‚å¦‚æœä¸ç›¸å…³ï¼Œåªå›ç­”ï¼šä¸ç›¸å…³"}
                ],
                max_tokens=100,
                temperature=0.3
            )
            answer = response.choices[0].message.content.strip()
            
            if "ä¸ç›¸å…³" in answer:
                return False, 0, ""
            
            # è§£æè¯„åˆ†å’Œå…³è”è¦ç‚¹
            if "|" in answer:
                parts = answer.split("|")
                try:
                    score = int(parts[0].strip())
                    relevance = parts[1].strip() if len(parts) > 1 else ""
                    return score >= 6, score, relevance  # 6åˆ†ä»¥ä¸Šè®¤ä¸ºç›¸å…³
                except:
                    return False, 0, ""
            
            return False, 0, ""
            
        except Exception as e:
            if attempt < max_attempts - 1:
                time.sleep(2)
                continue
            return False, 0, ""

# AI åˆ†ç±»è®ºæ–‡ç ”ç©¶æ–¹å‘ï¼ˆç”¨äºä»Šæ—¥é€Ÿé€’ï¼‰
def classify_paper_region(title, abstract, discipline, max_attempts=2):
    """
    è®© DeepSeek åˆ†æè®ºæ–‡å±äºå“ªä¸ªç ”ç©¶æ–¹å‘
    """
    topics_list = list(DISCIPLINES[discipline]["topics"].keys())
    topics_desc = "ã€".join(topics_list)
    
    for attempt in range(max_attempts):
        try:
            response = client_ai.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯åˆ†ç±»ä¸“å®¶ã€‚è¯·åˆ¤æ–­è®ºæ–‡æœ€è´´åˆä»¥ä¸‹å“ªä¸ªç ”ç©¶æ–¹å‘ã€‚"},
                    {"role": "user", "content": f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\n\næ‘˜è¦ï¼š{abstract}\n\nå€™é€‰æ–¹å‘ï¼š{topics_desc}\n\nè¯·ä»å€™é€‰æ–¹å‘ä¸­é€‰æ‹©ä¸€ä¸ªæœ€è´´åˆçš„æ–¹å‘ï¼Œåªå›ç­”æ–¹å‘åç§°ã€‚"}
                ],
                max_tokens=20,
                temperature=0.1
            )
            answer = response.choices[0].message.content.strip()
            
            # åŒ¹é…å…·ä½“æ–¹å‘
            for topic in topics_list:
                if topic in answer:
                    return topic
            
            # å¦‚æœæ²¡åŒ¹é…åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤
            return topics_list[0] if topics_list else "æœªåˆ†ç±»"
                
        except Exception as e:
            if attempt < max_attempts - 1:
                time.sleep(1)
                continue
            return "æœªåˆ†ç±»"
    
    return "æœªåˆ†ç±»"

# Semantic Scholar API - é¡¶åˆŠç›‘æ§
def fetch_top_journal_updates(discipline, topic_keywords, days=7):
    """
    ä» Semantic Scholar è·å–é¡¶åˆŠæœ€æ–°è®ºæ–‡ï¼ˆé»˜è®¤è¿‘7å¤©ï¼‰
    """
    target_venues = TOP_JOURNALS.get(discipline, [])
    current_year = datetime.now().year
    
    # æ„å»ºå…³é”®è¯æŸ¥è¯¢
    keyword_query = " ".join(topic_keywords[:5])  # ä½¿ç”¨å‰5ä¸ªå…³é”®è¯
    
    url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {
        "query": keyword_query,
        "year": f"{current_year}",  # åªæœå½“å‰å¹´ä»½
        "limit": 100,
        "fields": "title,abstract,venue,year,publicationDate,citationCount,url,authors"
    }
    
    # æ·»åŠ é‡è¯•æœºåˆ¶
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # å¢åŠ è¶…æ—¶æ—¶é—´å¹¶ç¦ç”¨ SSL éªŒè¯ï¼ˆåœ¨æŸäº›ç½‘ç»œç¯å¢ƒä¸‹éœ€è¦ï¼‰
            response = requests.get(
                url, 
                params=params, 
                timeout=30,
                verify=False  # ç¦ç”¨ SSL éªŒè¯ï¼ˆå¯èƒ½è§£å†³ SSL é”™è¯¯ï¼‰
            )
            
            # è°ƒè¯•ä¿¡æ¯
            st.info(f"ğŸ“¡ API è¯·æ±‚çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                if attempt < max_retries - 1:
                    st.warning(f"API è¯·æ±‚å¤±è´¥ï¼ˆçŠ¶æ€ç  {response.status_code}ï¼‰ï¼Œæ­£åœ¨é‡è¯• {attempt + 2}/{max_retries}...")
                    time.sleep(2)
                    continue
                else:
                    st.error(f"API è¿”å›é”™è¯¯: {response.status_code} - {response.text[:200]}")
                    return []
            
            data = response.json()
            st.info(f"ğŸ“Š API è¿”å›è®ºæ–‡æ€»æ•°: {len(data.get('data', []))}")
            
            cleaned_papers = []
            filtered_count = 0
            venue_debug = {}  # è°ƒè¯•ï¼šè®°å½•æ‰€æœ‰å‡ºç°çš„æœŸåˆŠå
            
            if 'data' in data:
                for paper in data['data']:
                    venue_name = paper.get('venue', '')
                    pub_date = paper.get('publicationDate', '')
                    
                    # è®°å½•æœŸåˆŠå‡ºç°æ¬¡æ•°
                    if venue_name:
                        venue_debug[venue_name] = venue_debug.get(venue_name, 0) + 1
                    
                    # æœŸåˆŠè¿‡æ»¤ - æ”¾å®½åŒ¹é…æ¡ä»¶
                    if not venue_name:
                        filtered_count += 1
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡æœŸåˆŠåˆ—è¡¨ä¸­ï¼ˆæ›´å®½æ¾çš„æ¨¡ç³ŠåŒ¹é…ï¼‰
                    is_target_venue = False
                    venue_lower = venue_name.lower()
                    for target in target_venues:
                        target_lower = target.lower()
                        # åªè¦åŒ…å«å…³é”®è¯å°±è®¤ä¸ºåŒ¹é…ï¼ˆæ¯”å¦‚ "Nature" åŒ¹é… "Nature Photonics" å’Œ "Nature Communications"ï¼‰
                        target_key = target_lower.split()[0]  # å–ç¬¬ä¸€ä¸ªå•è¯ä½œä¸ºå…³é”®è¯
                        if target_key in venue_lower or venue_lower in target_lower or target_lower in venue_lower:
                            is_target_venue = True
                            break
                    
                    if not is_target_venue:
                        filtered_count += 1
                        continue
                    
                    # æ—¶é—´è¿‡æ»¤ï¼ˆå¦‚æœæœ‰å‘è¡¨æ—¥æœŸï¼‰
                    if pub_date:
                        try:
                            pub_datetime = datetime.strptime(pub_date, '%Y-%m-%d')
                            if (datetime.now() - pub_datetime).days > days:
                                continue
                        except:
                            pass
                    
                    abstract_text = paper.get('abstract', '')
                    
                    # ç”Ÿæˆä¸€å¥è¯æ€»ç»“
                    one_sentence = ""
                    if abstract_text:
                        one_sentence = generate_one_sentence_summary(paper['title'], abstract_text)
                    
                    cleaned_papers.append({
                        "title": paper['title'],
                        "venue": venue_name,
                        "year": paper['year'],
                        "link": paper.get('url', ''),
                        "abstract": abstract_text,
                        "one_sentence": one_sentence,
                        "date": pub_date
                    })
            
            # æ˜¾ç¤ºæœŸåˆŠè°ƒè¯•ä¿¡æ¯
            if venue_debug:
                st.info(f"ğŸ“š æ£€æµ‹åˆ°çš„æœŸåˆŠï¼ˆå‰10ï¼‰: {', '.join(list(venue_debug.keys())[:10])}")
            st.info(f"ğŸ¯ æœŸåˆŠè¿‡æ»¤: å‰”é™¤ {filtered_count} ç¯‡éç›®æ ‡æœŸåˆŠï¼Œä¿ç•™ {len(cleaned_papers)} ç¯‡")
            return cleaned_papers
            
        except requests.exceptions.SSLError as e:
            if attempt < max_retries - 1:
                st.warning(f"âš ï¸ SSL è¿æ¥é”™è¯¯ï¼Œæ­£åœ¨é‡è¯• {attempt + 2}/{max_retries}... (å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜)")
                time.sleep(3)
                continue
            else:
                st.error(f"âŒ SSL è¿æ¥å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰ã€‚è¯·æ£€æŸ¥ï¼š\n1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n2. æ˜¯å¦ä½¿ç”¨äº†ä»£ç†æˆ–é˜²ç«å¢™\n3. ç¨åå†è¯•")
                return []
        except requests.exceptions.ConnectionError as e:
            if attempt < max_retries - 1:
                st.warning(f"âš ï¸ ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œæ­£åœ¨é‡è¯• {attempt + 2}/{max_retries}...")
                time.sleep(3)
                continue
            else:
                st.error(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚")
                return []
        except Exception as e:
            if attempt < max_retries - 1:
                st.warning(f"âš ï¸ è¯·æ±‚å‡ºé”™ï¼Œæ­£åœ¨é‡è¯• {attempt + 2}/{max_retries}...")
                time.sleep(3)
                continue
            else:
                st.error(f"âŒ Semantic Scholar API è°ƒç”¨å¤±è´¥ï¼š{str(e)[:200]}")
                return []
    
    return []

# arXiv ä»Šæ—¥é€Ÿé€’
def fetch_arxiv_daily_updates(discipline_config, hours=24):
    """
    è·å–è¿‡å» N å°æ—¶çš„ arXiv æ–°è®ºæ–‡
    """
    category_query = " OR ".join([f"cat:{cat}" for cat in discipline_config["arxiv_categories"]])
    
    client = arxiv.Client(page_size=10, delay_seconds=2, num_retries=3)
    search = arxiv.Search(
        query=category_query,
        max_results=50,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
    recent_papers = []
    
    try:
        for result in client.results(search):
            if result.published >= cutoff_time:
                recent_papers.append({
                    "title": result.title,
                    "pdf_url": result.pdf_url,
                    "published": result.published,
                    "summary": result.summary,
                    "authors": [author.name for author in result.authors]
                })
            else:
                break  # å·²ç»æŒ‰æ—¶é—´æ’åºï¼Œåé¢çš„æ›´æ—©
        
        return recent_papers
    except Exception as e:
        st.error(f"arXiv API è°ƒç”¨å¤±è´¥ï¼š{e}")
        return []

# å¼€å§‹ä¸»ç•Œé¢ - Tab æ¨¡å¼
tab1, tab2, tab3 = st.tabs(["ğŸ”¥ arXiv ä»Šæ—¥é€Ÿé€’", "ğŸ† é¡¶åˆŠå‘¨è®ºæ–‡", "ğŸ” è‡ªå®šä¹‰æœç´¢"])

# Tab 1: arXiv ä»Šæ—¥é€Ÿé€’
with tab1:
    st.subheader("ğŸ“¡ arXiv ä»Šæ—¥é€Ÿé€’ (Paper in 24hours)")
    
    # å­¦ç§‘é€‰æ‹©å’Œé™åˆ¶è®¾ç½®
    col1, col2, col3 = st.columns([2, 2, 2])
    with col1:
        daily_discipline = st.selectbox(
            "é€‰æ‹©å­¦ç§‘",
            options=list(DISCIPLINES.keys()),
            key="daily_discipline"
        )
    with col2:
        daily_limit_option = st.selectbox(
            "æœ€å¤§è®ºæ–‡ç¯‡æ•°",
            options=["æ— é™åˆ¶", "10ç¯‡", "20ç¯‡", "30ç¯‡", "50ç¯‡", "100ç¯‡"],
            index=3,
            key="daily_limit"
        )
        # è§£æé™åˆ¶æ•°é‡
        if daily_limit_option == "æ— é™åˆ¶":
            daily_max_papers = None
        else:
            daily_max_papers = int(daily_limit_option.replace("ç¯‡", ""))
    with col3:
        st.caption(f"ç›‘æ§åˆ†ç±»ï¼š{', '.join(DISCIPLINES[daily_discipline]['arxiv_categories'])}")
    
    if st.button("ğŸ”„ åˆ·æ–°ä»Šæ—¥é€Ÿé€’", key="refresh_daily", type="primary"):
        with st.spinner("æ­£åœ¨æ‰«æ arXiv æœ€æ–°è®ºæ–‡å¹¶åˆ†æåˆ†ç±»..."):
            discipline_config = DISCIPLINES[daily_discipline]
            daily_papers = fetch_arxiv_daily_updates(discipline_config, hours=24)
            
            if daily_papers:
                # åº”ç”¨ç¯‡æ•°é™åˆ¶
                if daily_max_papers is not None and len(daily_papers) > daily_max_papers:
                    daily_papers = daily_papers[:daily_max_papers]
                
                # ä½¿ç”¨ AI åˆ†ç±»æ¯ç¯‡è®ºæ–‡å¹¶ç”Ÿæˆä¸€å¥è¯æ€»ç»“
                region_counts = {}
                user_field = st.session_state.get('user_research_field', '')
                user_relevant_papers = []  # ä¸ç”¨æˆ·ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³çš„è®ºæ–‡
                
                for paper in daily_papers:
                    region = classify_paper_region(
                        paper['title'], 
                        paper['summary'], 
                        daily_discipline
                    )
                    paper['region'] = region
                    # ç”Ÿæˆä¸€å¥è¯æ€»ç»“
                    paper['one_sentence'] = generate_one_sentence_summary(paper['title'], paper['summary'])
                    region_counts[region] = region_counts.get(region, 0) + 1
                    
                    # AI ç²¾é€‰ï¼šåˆ¤æ–­æ˜¯å¦ä¸ç”¨æˆ·ç ”ç©¶é¢†åŸŸç›¸å…³
                    if user_field:
                        is_user_relevant, relevance_score, relevance_point = ai_filter_for_user(
                            paper['title'], 
                            paper['summary'], 
                            user_field
                        )
                        if is_user_relevant:
                            paper['user_relevant'] = True
                            paper['relevance_score'] = relevance_score
                            paper['relevance_point'] = relevance_point
                            user_relevant_papers.append(paper)
                
                st.success(f"âœ… æ‰¾åˆ° {len(daily_papers)} ç¯‡æ–°è®ºæ–‡ï¼" + (f" å…¶ä¸­ **{len(user_relevant_papers)}** ç¯‡ä¸æ‚¨çš„ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³ ğŸ¯" if user_relevant_papers else ""))
                
                # æ˜¾ç¤ºé¥¼å›¾
                if region_counts:
                    st.subheader("ğŸ“Š ç ”ç©¶æ–¹å‘åˆ†å¸ƒ")
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        import plotly.graph_objects as go
                        fig = go.Figure(data=[go.Pie(
                            labels=list(region_counts.keys()),
                            values=list(region_counts.values()),
                            hole=0.3,
                            textposition='inside',
                            textinfo='percent',
                            hovertemplate='<b>%{label}</b><br>è®ºæ–‡æ•°ï¼š%{value}<br>å æ¯”ï¼š%{percent}<extra></extra>'
                        )])
                        fig.update_layout(
                            showlegend=True,
                            height=400,
                            legend=dict(
                                orientation="v",
                                yanchor="middle",
                                y=0.5,
                                xanchor="left",
                                x=1.02,
                                font=dict(size=11)
                            ),
                            margin=dict(l=20, r=120, t=20, b=20)
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        st.markdown("**ç»Ÿè®¡æ•°æ®**")
                        for region, count in sorted(region_counts.items(), key=lambda x: x[1], reverse=True):
                            percentage = (count / len(daily_papers)) * 100
                            st.metric(region, f"{count} ç¯‡", f"{percentage:.1f}%")
                
                st.divider()
                
                # å¦‚æœæœ‰ç²¾é€‰è®ºæ–‡ï¼Œä¼˜å…ˆæ˜¾ç¤º
                if user_relevant_papers:
                    st.subheader(f"ğŸ¯ ä¸ºæ‚¨ç²¾é€‰ ({len(user_relevant_papers)} ç¯‡)")
                    st.caption("AI è¯†åˆ«å‡ºä¸æ‚¨ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³çš„è®ºæ–‡")
                    
                    for i, paper in enumerate(user_relevant_papers, 1):
                        hours_ago = int((datetime.now(timezone.utc) - paper['published']).total_seconds() / 3600)
                        with st.expander(f"â­ **{i}. {paper['title']}** ğŸ• {hours_ago}å°æ—¶å‰", expanded=(i==1)):
                            # æ˜¾ç¤ºç›¸å…³åº¦è¯„åˆ†å’Œå…³è”è¦ç‚¹
                            if paper.get('relevance_score'):
                                st.markdown(f"**ğŸ¯ ç›¸å…³åº¦è¯„åˆ†ï¼š** {paper['relevance_score']}/10")
                            if paper.get('relevance_point'):
                                st.info(f"ğŸ’¡ **ä¸æ‚¨çš„å…³è”ï¼š** {paper['relevance_point']}")
                            
                            # æ˜¾ç¤ºåˆ†ç±»æ ‡ç­¾
                            region_tag = paper.get('region', 'æœªåˆ†ç±»')
                            st.markdown(f"**ğŸ·ï¸ ç ”ç©¶æ–¹å‘ï¼š** `{region_tag}`")
                            st.markdown(f"**ğŸ‘¥ ä½œè€…ï¼š** {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                            st.markdown(f"**ğŸ”— PDFï¼š** [{paper['pdf_url']}]({paper['pdf_url']})")
                            
                            # æ˜¾ç¤ºä¸€å¥è¯æ€»ç»“
                            if paper.get('one_sentence'):
                                st.markdown(f"**ğŸ’¡ æ ¸å¿ƒå·¥ä½œï¼š** {paper['one_sentence']}")
                            
                            # æ˜¾ç¤ºå®Œæ•´è‹±æ–‡æ‘˜è¦
                            st.markdown("**ğŸ“ å®Œæ•´æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰ï¼š**")
                            with st.expander("å±•å¼€æŸ¥çœ‹å®Œæ•´æ‘˜è¦"):
                                st.caption(paper['summary'])
                    
                    st.divider()
                
                st.subheader("ğŸ“š å…¨éƒ¨è®ºæ–‡åˆ—è¡¨")
                
                for i, paper in enumerate(daily_papers, 1):
                    hours_ago = int((datetime.now(timezone.utc) - paper['published']).total_seconds() / 3600)
                    # å¦‚æœè®ºæ–‡å·²åœ¨ç²¾é€‰ä¸­æ˜¾ç¤ºï¼Œæ·»åŠ æ ‡è®°
                    title_prefix = "â­ " if paper.get('user_relevant') else ""
                    with st.expander(f"{title_prefix}**{i}. {paper['title']}** ğŸ• {hours_ago}å°æ—¶å‰"):
                        # æ˜¾ç¤ºåˆ†ç±»æ ‡ç­¾
                        region_tag = paper.get('region', 'æœªåˆ†ç±»')
                        st.markdown(f"**ğŸ·ï¸ ç ”ç©¶æ–¹å‘ï¼š** `{region_tag}`")
                        st.markdown(f"**ğŸ‘¥ ä½œè€…ï¼š** {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                        st.markdown(f"**ğŸ”— PDFï¼š** [{paper['pdf_url']}]({paper['pdf_url']})")
                        
                        # æ˜¾ç¤ºä¸€å¥è¯æ€»ç»“
                        if paper.get('one_sentence'):
                            st.markdown(f"**ğŸ’¡ æ ¸å¿ƒå·¥ä½œï¼š** {paper['one_sentence']}")
                        
                        # æ˜¾ç¤ºå®Œæ•´è‹±æ–‡æ‘˜è¦
                        st.markdown("**ğŸ“ å®Œæ•´æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰ï¼š**")
                        with st.expander("å±•å¼€æŸ¥çœ‹å®Œæ•´æ‘˜è¦"):
                            st.caption(paper['summary'])
            else:
                st.info("æš‚æ— æ–°è®ºæ–‡ï¼Œç¨åå†è¯•")
    else:
        st.info("ğŸ‘† ç‚¹å‡»æŒ‰é’®è·å–ä»Šæ—¥æœ€æ–°è®ºæ–‡")

# Tab 2: é¡¶åˆŠè¿‘ä¸€å‘¨è®ºæ–‡
with tab2:
    st.subheader("ğŸ† é¡¶åˆŠè¿‘ä¸€å‘¨ - æœ€æ–°å‘è¡¨ï¼ˆè¿‡å»7å¤©ï¼‰")
    
    # å­¦ç§‘å’Œæ–¹å‘é€‰æ‹©
    col1, col2 = st.columns(2)
    with col1:
        journal_discipline = st.selectbox(
            "é€‰æ‹©å­¦ç§‘",
            options=list(DISCIPLINES.keys()),
            key="journal_discipline"
        )
    
    with col2:
        journal_available_topics = DISCIPLINES[journal_discipline]["topics"]
        journal_selected_topics = st.multiselect(
            "é€‰æ‹©ç ”ç©¶æ–¹å‘",
            options=list(journal_available_topics.keys()),
            default=[list(journal_available_topics.keys())[0]],
            key="journal_topics"
        )
    
    st.caption(f"ğŸ“š ç›‘æ§æœŸåˆŠï¼š{', '.join(TOP_JOURNALS[journal_discipline])}")
    
    if journal_selected_topics and st.button("ğŸ” æœç´¢é¡¶åˆŠè®ºæ–‡", key="search_journals", type="primary"):
        with st.spinner("æ­£åœ¨æ‰«æé¡¶çº§æœŸåˆŠè¿‘ä¸€å‘¨è®ºæ–‡..."):
            all_keywords = []
            for topic in journal_selected_topics:
                all_keywords.extend(journal_available_topics[topic]["keywords"])
            
            journal_papers = fetch_top_journal_updates(journal_discipline, all_keywords, days=7)
            
            if journal_papers:
                # AI ç²¾é€‰ï¼šåˆ¤æ–­ä¸ç”¨æˆ·ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§
                user_field = st.session_state.get('user_research_field', '')
                user_relevant_papers = []
                
                if user_field:
                    for paper in journal_papers:
                        is_user_relevant, relevance_score, relevance_point = ai_filter_for_user(
                            paper['title'], 
                            paper['abstract'], 
                            user_field
                        )
                        if is_user_relevant:
                            paper['user_relevant'] = True
                            paper['relevance_score'] = relevance_score
                            paper['relevance_point'] = relevance_point
                            user_relevant_papers.append(paper)
                
                st.success(f"âœ… æ‰¾åˆ° {len(journal_papers)} ç¯‡é¡¶åˆŠè®ºæ–‡ï¼" + (f" å…¶ä¸­ **{len(user_relevant_papers)}** ç¯‡ä¸æ‚¨çš„ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³ ğŸ¯" if user_relevant_papers else ""))
                
                # å¦‚æœæœ‰ç²¾é€‰è®ºæ–‡ï¼Œä¼˜å…ˆæ˜¾ç¤º
                if user_relevant_papers:
                    st.subheader(f"ğŸ¯ ä¸ºæ‚¨ç²¾é€‰ ({len(user_relevant_papers)} ç¯‡)")
                    st.caption("AI è¯†åˆ«å‡ºä¸æ‚¨ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³çš„é¡¶åˆŠè®ºæ–‡")
                    
                    for i, paper in enumerate(user_relevant_papers, 1):
                        with st.expander(f"â­ **{i}. {paper['title']}**", expanded=(i==1)):
                            # æ˜¾ç¤ºç›¸å…³åº¦è¯„åˆ†å’Œå…³è”è¦ç‚¹
                            if paper.get('relevance_score'):
                                st.markdown(f"**ğŸ¯ ç›¸å…³åº¦è¯„åˆ†ï¼š** {paper['relevance_score']}/10")
                            if paper.get('relevance_point'):
                                st.info(f"ğŸ’¡ **ä¸æ‚¨çš„å…³è”ï¼š** {paper['relevance_point']}")
                            
                            st.markdown(f"**ğŸ“š æœŸåˆŠï¼š** {paper['venue']} ({paper['year']})")
                            st.markdown(f"**ğŸ“… å‘è¡¨æ—¶é—´ï¼š** {paper['date']}")
                            st.markdown(f"**ğŸ”— é“¾æ¥ï¼š** [{paper['link']}]({paper['link']})")
                            
                            # æ˜¾ç¤ºä¸€å¥è¯æ€»ç»“
                            if paper.get('one_sentence'):
                                st.markdown(f"**ğŸ’¡ æ ¸å¿ƒå·¥ä½œï¼š** {paper['one_sentence']}")
                            
                            # æ˜¾ç¤ºå®Œæ•´è‹±æ–‡æ‘˜è¦
                            if paper['abstract']:
                                st.markdown("**ğŸ“ å®Œæ•´æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰ï¼š**")
                                with st.expander("å±•å¼€æŸ¥çœ‹å®Œæ•´æ‘˜è¦"):
                                    st.caption(paper['abstract'])
                    
                    st.divider()
                
                st.subheader("ğŸ“š å…¨éƒ¨é¡¶åˆŠè®ºæ–‡åˆ—è¡¨")
                
                for i, paper in enumerate(journal_papers, 1):
                    # å¦‚æœè®ºæ–‡å·²åœ¨ç²¾é€‰ä¸­æ˜¾ç¤ºï¼Œæ·»åŠ æ ‡è®°
                    title_prefix = "â­ " if paper.get('user_relevant') else ""
                    with st.expander(f"{title_prefix}**{i}. {paper['title']}**"):
                        st.markdown(f"**ğŸ“š æœŸåˆŠï¼š** {paper['venue']} ({paper['year']})")
                        st.markdown(f"**ğŸ“… å‘è¡¨æ—¶é—´ï¼š** {paper['date']}")
                        st.markdown(f"**ï¿½ é“¾æ¥ï¼š** [{paper['link']}]({paper['link']})")
                        
                        # æ˜¾ç¤ºä¸€å¥è¯æ€»ç»“
                        if paper.get('one_sentence'):
                            st.markdown(f"**ğŸ’¡ æ ¸å¿ƒå·¥ä½œï¼š** {paper['one_sentence']}")
                        
                        # æ˜¾ç¤ºå®Œæ•´è‹±æ–‡æ‘˜è¦
                        if paper['abstract']:
                            st.markdown("**ğŸ“ å®Œæ•´æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰ï¼š**")
                            with st.expander("å±•å¼€æŸ¥çœ‹å®Œæ•´æ‘˜è¦"):
                                st.caption(paper['abstract'])
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œå°è¯•è°ƒæ•´ç ”ç©¶æ–¹å‘")
    else:
        if not journal_selected_topics:
            st.warning("âš ï¸ è¯·å…ˆé€‰æ‹©ç ”ç©¶æ–¹å‘")
        else:
            st.info("ğŸ‘† ç‚¹å‡»æŒ‰é’®æœç´¢é¡¶åˆŠæœ€æ–°è®ºæ–‡")

# Tab 3: è‡ªå®šä¹‰æœç´¢ï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰
with tab3:
    st.subheader("ğŸ” è‡ªå®šä¹‰æ·±åº¦æœç´¢")
    st.caption("ä½¿ç”¨ AI æ™ºèƒ½ç­›é€‰ï¼Œç”Ÿæˆä¸­æ–‡æ‘˜è¦")
    
    # æœç´¢æ¨¡å¼é€‰æ‹©
    search_mode = st.radio(
        "æœç´¢æ¨¡å¼",
        options=["ğŸ“‹ ä»é¢„è®¾é¢†åŸŸé€‰æ‹©", "âœï¸ è‡ªå®šä¹‰è¾“å…¥å…³é”®è¯"],
        horizontal=True,
        key="search_mode"
    )
    
    st.markdown("---")
    
    if search_mode == "ğŸ“‹ ä»é¢„è®¾é¢†åŸŸé€‰æ‹©":
        # åŸæœ‰çš„é¢„è®¾é¢†åŸŸé€‰æ‹©
        col1, col2 = st.columns(2)
        with col1:
            custom_discipline = st.selectbox(
                "é€‰æ‹©å­¦ç§‘",
                options=list(DISCIPLINES.keys()),
                key="custom_discipline"
            )
        
        with col2:
            custom_available_topics = DISCIPLINES[custom_discipline]["topics"]
            custom_selected_topics = st.multiselect(
                "é€‰æ‹©ç ”ç©¶æ–¹å‘",
                options=list(custom_available_topics.keys()),
                default=list(custom_available_topics.keys())[:2],
                key="custom_topics"
            )
    else:
        # è‡ªå®šä¹‰è¾“å…¥æ¨¡å¼
        col1, col2 = st.columns(2)
        with col1:
            custom_discipline = st.selectbox(
                "é€‰æ‹©å­¦ç§‘åˆ†ç±»ï¼ˆç”¨äºé™å®šarXivç±»åˆ«ï¼‰",
                options=list(DISCIPLINES.keys()),
                key="custom_discipline_manual"
            )
        
        with col2:
            st.caption("ğŸ’¡ æç¤ºï¼šå¯è¾“å…¥å¤šä¸ªå…³é”®è¯ï¼Œç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”")
        
        custom_keywords_input = st.text_area(
            "è¾“å…¥æœç´¢å…³é”®è¯",
            placeholder="ä¾‹å¦‚ï¼šultraviolet laser, 355nm, 266nm, solid-state laser, lithography",
            height=100,
            key="custom_keywords"
        )
        
        # å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºå…³é”®è¯åˆ—è¡¨
        if custom_keywords_input:
            # æ”¯æŒé€—å·æˆ–ç©ºæ ¼åˆ†éš”
            custom_keywords_list = [kw.strip() for kw in custom_keywords_input.replace(',', ' ').split() if kw.strip()]
        else:
            custom_keywords_list = []
    
    # æ—¶é—´å’Œæ•°é‡è®¾ç½®
    col3, col4 = st.columns(2)
    with col3:
        custom_days_range = st.slider(
            "æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰",
            min_value=1,
            max_value=30,
            value=7,
            key="custom_days"
        )
    
    with col4:
        custom_max_results = st.number_input(
            "æœ€å¤§æ£€ç´¢æ•°é‡",
            min_value=10,
            max_value=500,
            value=100,
            step=10,
            key="custom_max"
        )
    
    st.markdown("---")
    
    # å¼€å§‹æœç´¢æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æœç´¢", type="primary", key="custom_search"):
        # æ ¹æ®æœç´¢æ¨¡å¼æ„å»ºå…³é”®è¯
        if search_mode == "ğŸ“‹ ä»é¢„è®¾é¢†åŸŸé€‰æ‹©":
            # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†ç ”ç©¶æ–¹å‘
            if not custom_selected_topics:
                st.error("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼")
                st.stop()
            
            # è·å–å½“å‰å­¦ç§‘çš„é…ç½®
            discipline_config = DISCIPLINES[custom_discipline]
            available_topics = discipline_config["topics"]
            
            # æ ¹æ®é€‰æ‹©çš„ç ”ç©¶æ–¹å‘æ„å»ºæœç´¢å…³é”®è¯
            all_keywords = []
            for topic in custom_selected_topics:
                all_keywords.extend(available_topics[topic]["keywords"])
        else:
            # è‡ªå®šä¹‰å…³é”®è¯æ¨¡å¼
            if not custom_keywords_list:
                st.error("âŒ è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªæœç´¢å…³é”®è¯ï¼")
                st.stop()
            
            # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯
            all_keywords = custom_keywords_list
            discipline_config = DISCIPLINES[custom_discipline]
        
        # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ˆç”¨ OR è¿æ¥æ‰€æœ‰å…³é”®è¯ï¼‰
        keyword_query = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in all_keywords])
        
        # æ ¹æ®å­¦ç§‘æ„å»º arXiv åˆ†ç±»æŸ¥è¯¢
        category_query = " OR ".join([f"cat:{cat}" for cat in discipline_config["arxiv_categories"]])
        full_query = f"({category_query}) AND ({keyword_query})"
        
        # é…ç½® arXiv å®¢æˆ·ç«¯
        client = arxiv.Client(page_size=10, delay_seconds=3, num_retries=5)
        search = arxiv.Search(
            query=full_query,
            max_results=custom_max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        today = datetime.now(timezone.utc)
        
        # è¿›åº¦æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        count = 0
        filtered_count = 0
        relevant_papers = []
        
        try:
            results_list = list(client.results(search))
            total = len(results_list)
            
            for idx, result in enumerate(results_list):
                count += 1
                time_diff = (today - result.published).days
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress((idx + 1) / total)
                status_text.text(f"æ­£åœ¨æ£€æŸ¥ç¬¬ {count} ç¯‡è®ºæ–‡... ({time_diff} å¤©å‰)")
                
                if time_diff > custom_days_range:
                    continue
                
                filtered_count += 1
                
                # åˆ¤æ–­ç›¸å…³æ€§
                if search_mode == "ğŸ“‹ ä»é¢„è®¾é¢†åŸŸé€‰æ‹©":
                    # ä½¿ç”¨åŸæœ‰çš„ç›¸å…³æ€§åˆ¤æ–­
                    is_related, related_topics = is_relevant_with_tags(result.title, result.summary, custom_selected_topics)
                else:
                    # è‡ªå®šä¹‰å…³é”®è¯æ¨¡å¼ï¼šåªè¦æ£€ç´¢åˆ°å°±è®¤ä¸ºç›¸å…³
                    is_related = True
                    related_topics = [f"åŒ¹é…å…³é”®è¯: {', '.join(all_keywords[:3])}"]
                
                if is_related:
                    # ç”Ÿæˆä¸­æ–‡æ‘˜è¦
                    summary_cn = generate_summary(result.title, result.summary)
                    
                    relevant_papers.append({
                        'title': result.title,
                        'pdf_url': result.pdf_url,
                        'published': result.published,
                        'summary': summary_cn,
                        'summary_en': result.summary,
                        'related_topics': related_topics
                    })
                
                time.sleep(0.5)
            
            progress_bar.empty()
            status_text.empty()
            
            # æ˜¾ç¤ºç»“æœ
            st.success(f"âœ… æœç´¢å®Œæˆï¼å…±æ£€æŸ¥ {count} ç¯‡è®ºæ–‡ï¼Œ{filtered_count} ç¯‡åœ¨æ—¶é—´èŒƒå›´å†…ï¼Œæ‰¾åˆ° {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡ã€‚")
            
            if relevant_papers:
                # AI ç²¾é€‰ï¼šåˆ¤æ–­ä¸ç”¨æˆ·ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§
                user_field = st.session_state.get('user_research_field', '')
                user_relevant_papers = []
                
                if user_field:
                    for paper in relevant_papers:
                        is_user_relevant, relevance_score, relevance_point = ai_filter_for_user(
                            paper['title'], 
                            paper['summary_en'], 
                            user_field
                        )
                        if is_user_relevant:
                            paper['user_relevant'] = True
                            paper['relevance_score'] = relevance_score
                            paper['relevance_point'] = relevance_point
                            user_relevant_papers.append(paper)
                
                if user_relevant_papers:
                    st.success(f"ğŸ¯ å…¶ä¸­ **{len(user_relevant_papers)}** ç¯‡ä¸æ‚¨çš„ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³")
                
                st.markdown("---")
                
                # å¦‚æœæœ‰ç²¾é€‰è®ºæ–‡ï¼Œä¼˜å…ˆæ˜¾ç¤º
                if user_relevant_papers:
                    st.subheader(f"ğŸ¯ ä¸ºæ‚¨ç²¾é€‰ ({len(user_relevant_papers)} ç¯‡)")
                    st.caption("AI è¯†åˆ«å‡ºä¸æ‚¨ç ”ç©¶é¢†åŸŸé«˜åº¦ç›¸å…³çš„è®ºæ–‡")
                    
                    for i, paper in enumerate(user_relevant_papers, 1):
                        with st.expander(f"â­ **{i}. {paper['title']}**", expanded=(i==1)):
                            # æ˜¾ç¤ºç›¸å…³åº¦è¯„åˆ†å’Œå…³è”è¦ç‚¹
                            if paper.get('relevance_score'):
                                st.markdown(f"**ğŸ¯ ç›¸å…³åº¦è¯„åˆ†ï¼š** {paper['relevance_score']}/10")
                            if paper.get('relevance_point'):
                                st.info(f"ğŸ’¡ **ä¸æ‚¨çš„å…³è”ï¼š** {paper['relevance_point']}")
                            
                            st.markdown(f"**ğŸ“… å‘å¸ƒæ—¶é—´ï¼š** {paper['published'].strftime('%Y-%m-%d')}")
                            
                            # æ˜¾ç¤º AI åˆ¤å®šçš„ç›¸å…³æ–¹å‘æ ‡ç­¾
                            if paper.get('related_topics'):
                                tags_html = " ".join([f"<span style='background-color: #e8f4f8; padding: 4px 12px; border-radius: 12px; margin-right: 8px; font-size: 14px;'>ğŸ·ï¸ {topic}</span>" for topic in paper['related_topics']])
                                st.markdown(f" {tags_html}", unsafe_allow_html=True)
                            
                            st.markdown(f"**ğŸ”— PDF é“¾æ¥ï¼š** [{paper['pdf_url']}]({paper['pdf_url']})")
                            st.markdown("**ğŸ“ AI ç”Ÿæˆæ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰ï¼š**")
                            st.info(paper['summary'])
                            
                            # æ˜¾ç¤ºå®Œæ•´è‹±æ–‡æ‘˜è¦
                            st.markdown("**ğŸ“„ å®Œæ•´æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰ï¼š**")
                            with st.expander("å±•å¼€æŸ¥çœ‹å®Œæ•´æ‘˜è¦"):
                                st.caption(paper.get('summary_en', ''))
                    
                    st.divider()
                
                st.header(f"ğŸ“š æ‰¾åˆ° {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
                
                for i, paper in enumerate(relevant_papers, 1):
                    # å¦‚æœè®ºæ–‡å·²åœ¨ç²¾é€‰ä¸­æ˜¾ç¤ºï¼Œæ·»åŠ æ ‡è®°
                    title_prefix = "â­ " if paper.get('user_relevant') else ""
                    with st.expander(f"{title_prefix}**{i}. {paper['title']}**", expanded=(i==1 and not user_relevant_papers)):
                        st.markdown(f"**ğŸ“… å‘å¸ƒæ—¶é—´ï¼š** {paper['published'].strftime('%Y-%m-%d')}")
                        
                        # æ˜¾ç¤º AI åˆ¤å®šçš„ç›¸å…³æ–¹å‘æ ‡ç­¾
                        if paper.get('related_topics'):
                            tags_html = " ".join([f"<span style='background-color: #e8f4f8; padding: 4px 12px; border-radius: 12px; margin-right: 8px; font-size: 14px;'>ğŸ·ï¸ {topic}</span>" for topic in paper['related_topics']])
                            st.markdown(f" {tags_html}", unsafe_allow_html=True)
                        
                        st.markdown(f"**ğŸ”— PDF é“¾æ¥ï¼š** [{paper['pdf_url']}]({paper['pdf_url']})")
                        st.markdown("**ğŸ“ AI ç”Ÿæˆæ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰ï¼š**")
                        st.info(paper['summary'])
                        
                        # æ˜¾ç¤ºå®Œæ•´è‹±æ–‡æ‘˜è¦
                        st.markdown("**ğŸ“„ å®Œæ•´æ‘˜è¦ï¼ˆè‹±æ–‡ï¼‰ï¼š**")
                        with st.expander("å±•å¼€æŸ¥çœ‹å®Œæ•´æ‘˜è¦"):
                            st.caption(paper.get('summary_en', ''))
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚")
        
        except Exception as e:
            st.error(f"âŒ æœç´¢å‡ºé”™ï¼š{e}")
            st.info("**å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š**\n1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n2. æ£€æŸ¥ DeepSeek API Key æ˜¯å¦æœ‰æ•ˆ\n3. ç¨åé‡è¯•")
    else:
        # æ˜¾ç¤ºå½“å‰è®¾ç½®
        if search_mode == "ğŸ“‹ ä»é¢„è®¾é¢†åŸŸé€‰æ‹©":
            if custom_selected_topics:
                topics_display = "ã€".join(custom_selected_topics)
                st.info(f"ğŸ‘† ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹æœç´¢{custom_discipline}è®ºæ–‡\n\n**å½“å‰è®¾ç½®ï¼š**\n- å­¦ç§‘é¢†åŸŸï¼š{DISCIPLINES[custom_discipline]['icon']} {custom_discipline}\n- ç ”ç©¶æ–¹å‘ï¼š{topics_display}\n- æ—¶é—´èŒƒå›´ï¼šè¿‡å» **{custom_days_range}** å¤©\n- æœ€å¤šæ£€ç´¢ï¼š**{custom_max_results}** ç¯‡")
            else:
                st.warning("âš ï¸ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç ”ç©¶æ–¹å‘")
        else:
            if custom_keywords_list:
                keywords_display = "ã€".join(custom_keywords_list[:5]) + ("..." if len(custom_keywords_list) > 5 else "")
                st.info(f"ğŸ‘† ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹æœç´¢\n\n**å½“å‰è®¾ç½®ï¼š**\n- å­¦ç§‘åˆ†ç±»ï¼š{DISCIPLINES[custom_discipline]['icon']} {custom_discipline}\n- æœç´¢å…³é”®è¯ï¼š{keywords_display}\n- å…³é”®è¯æ•°é‡ï¼š**{len(custom_keywords_list)}** ä¸ª\n- æ—¶é—´èŒƒå›´ï¼šè¿‡å» **{custom_days_range}** å¤©\n- æœ€å¤šæ£€ç´¢ï¼š**{custom_max_results}** ç¯‡")
            else:
                st.warning("âš ï¸ è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªæœç´¢å…³é”®è¯")
